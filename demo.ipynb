{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "SEED = 42\n",
    "EPS = 1e-5\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "target = wn.synset('mammal.n.01')\n",
    "words = wn.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82115 nouns\n",
      "6542 hypernyms\n"
     ]
    }
   ],
   "source": [
    "nouns = set()\n",
    "for word in words:\n",
    "    nouns.update(wn.synsets(word, pos='n'))\n",
    "\n",
    "print(len(nouns), 'nouns')\n",
    "\n",
    "hypernyms = []\n",
    "for noun in nouns:\n",
    "    paths = noun.hypernym_paths()\n",
    "    for path in paths:\n",
    "        try:\n",
    "            pos = path.index(target)\n",
    "            for i in range(pos, len(path)-1):\n",
    "                hypernyms.append((noun, path[i]))\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "hypernyms = np.array(list(set(hypernyms)))\n",
    "uniq_hypernyms = np.array(list(set([e for tup in hypernyms for e in tup])))\n",
    "\n",
    "word2idx = {val: i for i, val in enumerate(uniq_hypernyms)}\n",
    "random.shuffle(hypernyms)\n",
    "\n",
    "print(len(hypernyms), 'hypernyms' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj(params):\n",
    "    norm = params.norm(p=2, dim=1).unsqueeze(1)\n",
    "    norm[norm < 1] = 1+EPS\n",
    "    params = params.div(norm-EPS)\n",
    "    return params\n",
    "\n",
    "def arcosh(x):\n",
    "    return torch.log(x + torch.sqrt(x**2 - 1))\n",
    "\n",
    "def distance(u, v):\n",
    "    uu = v.norm(dim=1)**2\n",
    "    uu.register_hook(lambda grad: print('GRAD UU:', grad))\n",
    "    vv = v.norm(dim=1)**2\n",
    "    vv.register_hook(lambda grad: print('GRAD VV:', grad))\n",
    "    uv = u.mm(v.t())\n",
    "    alpha = 1-uu\n",
    "    alpha = alpha.clamp(min=EPS)\n",
    "    alpha.register_hook(lambda grad: print('GRAD ALPHA:', alpha))\n",
    "    beta = 1-vv\n",
    "    beta = beta.clamp(min=EPS)\n",
    "    #print(alpha, beta)\n",
    "    gamma = 1 + 2 * (uu - 2 * uv + vv) / (alpha * beta)\n",
    "    gamma = gamma.clamp(min=1)\n",
    "    #print('gamma', gamma)\n",
    "    \n",
    "    return arcosh(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "DIM = 2\n",
    "START_LR = 0.1\n",
    "FINAL_LR = 0.0001\n",
    "NEG = 10\n",
    "\n",
    "EMBEDDINGS = torch.Tensor(len(uniq_hypernyms), DIM)\n",
    "nn.init.uniform(EMBEDDINGS, a=-0.001, b=0.001)\n",
    "\n",
    "NEG_SAMPLES = torch.from_numpy(np.random.randint(0, len(uniq_hypernyms), size=(EPOCHS, len(hypernyms), NEG)))\n",
    "\n",
    "bar1 = tqdm_notebook(range(EPOCHS))\n",
    "for epoch in bar1:\n",
    "    #plot()\n",
    "    bar2 = tqdm_notebook(hypernyms, leave=False)\n",
    "    for i, (w1, w2) in enumerate(bar2):\n",
    "        i_w1 = word2idx[w1]\n",
    "        i_w2 = word2idx[w2]\n",
    "        u = Variable(EMBEDDINGS[i_w1].unsqueeze(0), requires_grad=True)\n",
    "        v = Variable(EMBEDDINGS[i_w2].unsqueeze(0), requires_grad=True)\n",
    "        negs = Variable(EMBEDDINGS[NEG_SAMPLES[epoch, i]], requires_grad=True)\n",
    "        \n",
    "        loss = torch.exp(-1*distance(u, v)) / torch.exp(-1*distance(u, negs)).sum()\n",
    "        bar2.set_postfix(loss=loss.data[0, 0])\n",
    "        loss.backward()\n",
    "        \n",
    "        if True in np.isnan(u.grad.data.numpy()):\n",
    "            '''print('u', u)\n",
    "            print('v', v)\n",
    "            print(torch.exp(-distance(u, v)))\n",
    "            print(torch.exp(-distance(u, negs)))\n",
    "            print(loss)\n",
    "            print(u.grad.data)'''\n",
    "            print('NaN in u grad')\n",
    "            break\n",
    "        if True in np.isnan(v.grad.data.numpy()):\n",
    "            print('NaN in v grad')\n",
    "            break\n",
    "        if True in np.isnan(negs.grad.data.numpy()):\n",
    "            print('NaN in negs grad')\n",
    "            break\n",
    "        \n",
    "        r = epoch/EPOCHS\n",
    "        LR = (1-r) * START_LR + r * FINAL_LR\n",
    "        EMBEDDINGS[NEG_SAMPLES[epoch, i]] -= LR * (((1-negs.norm(dim=1)**2) ** 2) / 4).data.unsqueeze(1) * negs.grad.data\n",
    "        EMBEDDINGS[i_w1] -= LR * (((1-u.norm()**2) **2) / 4).data * u.grad.data\n",
    "        EMBEDDINGS[i_w2] -= LR * (((1-v.norm()**2) **2) / 4).data * v.grad.data\n",
    "        \n",
    "        EMBEDDINGS = proj(EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot():\n",
    "    lhds, rhds = hypernyms[:, 0], hypernyms[:, 1]\n",
    "\n",
    "    targets = set(lhd for i, lhd in enumerate(lhds) if rhds[i] == target)\n",
    "    embeddings = EMBEDDINGS.numpy()\n",
    "\n",
    "    if len(targets) + 1 > 30:\n",
    "        targets = random.sample(targets, 30-1)\n",
    "    targets.append(target)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.gca()\n",
    "    ax.cla() # clear things for fresh plot\n",
    "\n",
    "    ax.set_xlim((-1.1, 1.1))\n",
    "    ax.set_ylim((-1.1, 1.1))\n",
    "\n",
    "    circle = plt.Circle((0,0), 1., color='black', fill=False)\n",
    "    ax.add_artist(circle)\n",
    "\n",
    "    for n in targets:\n",
    "        x, y = embeddings[word2idx[n]]\n",
    "        ax.plot(x, y, 'o', color='y')\n",
    "        ax.text(x+0.01, y+0.01, n, color='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
